

项目文档: kline-memory-db 服务

版本: 5.3 (内存加载优化版)
负责人: (你的名字)
最后更新: 2025-07-16

0. 系统架构概览
Generated code
+-------------+                                        +-------------------+
| Binance API |<---------------------------------------|  klines.db (SQLite) |
+-------------+                                        +----------+--------+
      ^                                                            |
      | (2. 增量数据下载)                                            | (4. 按需加载历史数据)
      |                                                            |
+-----+------------------------------------------------------------+-----+
|                                                                        |
|                             kline-memory-db (本服务)                     |
|                                                                        |
|  +-------------------------------------------------------------------+ |
|  | 1. 获取全品种, 生成稳定索引                                         | |
|  | 3. 执行数据补齐 (Backfill) & 直接加载                               | |
|  | 5. 启动Actor, 监听实时数据与查询                                  | |
|  +-------------------------------------------------------------------+ |
|                                                                        |
+--------------------------------+---------------------------------------+
                                 |
     (来自kline-aggregator)      | (对外提供查询)
                                 v
+-----------------------+      +---------------+
| Named Pipe (实时增量) |      |  API Consumers|
+-----------------------+      +---------------+


本篇文档专注于描述 kline-memory-db 服务的内部设计与实现。

1. 服务定位与核心职责

kline-memory-db 是一个高性能、自包含的K线内存数据库服务。它在启动阶段负责与权威数据源（币安API）和本地缓存（SQLite）同步，确保数据完整性；在运行阶段则作为整个交易数据系统的“历史数据中心”和“准实时查询引擎”。

核心职责:

自我完备的数据初始化: 在启动时，执行一个完整的数据同步与补齐流程，确保在服务就绪时，其拥有的数据是最新、最完整的。

构建并维护内存数据库: 将补齐后的历史K线数据加载到一个为极致性能而优化的内存结构中。

消费实时增量数据: 监听由 kline-aggregator 通过命名管道推送的实时增量K线数据，并将其追加到内存数据库中。

提供数据查询服务: 通过内部的Actor模型，响应外部（如HTTP API服务器）的数据查询请求。

数据持久化: 定期将内存中的增量数据变更备份到本地SQLite数据库。

2. 核心架构设计

本服务采用单线程无锁Actor模型与逻辑二维的句柄/指针矩阵数据存储相结合的设计，以实现最高的代码清晰度、健壮性和出色的性能。

2.1. 核心数据结构：基于稳定排序的指针矩阵

数据库的核心是一个嵌套的Vec结构 Vec<Box<Vec<Box<VecDeque<Kline>>>>>，它在逻辑上完美地映射了一个[品种][周期]的二维矩阵。

关键设计：稳定的全局索引 symbol_index

为了确保每次服务重启后，同一个交易对（如 BTCUSDT）总是在 all_klines 的相同索引位置，我们设计了一套基于上币时间的全局排序算法：

数据源: 获取所有交易对的日线数据。

主排序键: 以每个品种**最早一根日线K线的开盘时间 (open_time)**作为主要排序依据，时间越早，索引越靠前。

次排序键: 如果上币时间相同，则按品种的字母顺序（ASCII码）进行排序（例如 BTCUSDT 在 ETHUSDT 之前）。

稳定性: 这个排序算法的结果是确定且永久不变的。新币种的上币时间必然最晚，因此会自然地被追加到索引列表的末尾，不会破坏现有索引结构。

这个 symbol_index 是连接外部请求（如 "查询BTCUSDT"）和内部数据 all_klines[<index>] 的桥梁。

Generated rust
// actor_state.rs - Actor内部持有的数据
struct DbActorState {
    // ...
    // all_klines 的第一维索引 (symbol_index) 是基于上币时间稳定排序的。
    all_klines: Vec<Box<Vec<Box<VecDeque<Kline>>>>>,
    // 用于 "BTCUSDT" -> symbol_index 的快速查找
    symbol_to_index: HashMap<String, usize>,
    // ...
}
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Rust
IGNORE_WHEN_COPYING_END
2.2. 并发安全与通信

Actor模型: 采用单线程Actor模型，所有数据访问通过MPSC通道串行处理，从根本上避免了数据竞争和锁的需求。

与聚合器通信: 通过Windows命名管道接收来自 kline-aggregator 的实时K线。

3. 启动与初始化流程

服务的启动是一个阻塞式的、精心编排的六步流程，以确保服务一旦可用，其数据就是完整且准确的。

获取全量品种列表: 调用币安API (get_trading_usdt_perpetual_symbols)，获取当前所有可交易的U本位永续合约列表。

生成稳定全局索引:

并发下载所有品种的日线K线数据。

根据 2.1节 中描述的“上币时间优先，字母顺序其次”的规则，对所有品种进行排序，生成全局索引。

初始化内存与数据库结构:

根据品种总数，分配好内存数据库的核心Vec结构 (all_klines)。

遍历所有品种和周期，确保SQLite中对应的物理表都已创建。

并行化数据补齐与持久化:

此步骤利用 backfill.rs 逻辑，高并发处理所有品种的所有周期。

对于每个处理单元（例如 BTCUSDT 的 1h 周期）：
a. 读取本地SQLite中该单元的最新K线时间戳。
b. 计算需补充的时间范围，创建下载任务。
c. 从币安API下载所有缺失的K线。
d. 将这些新下载的K线 UPSERT 到SQLite数据库中，完成持久化。

关键: 此步骤完成后，所有新下载的K线数据同时保留在程序的内存中，为下一步高效加载做准备。

高效加载数据到内存:

此步骤与第4步紧密衔接，在数据补齐后立即执行，以避免不必要的磁盘读取。

对于每一个品种周期（例如 BTCUSDT 的 1h）：
a. 优先使用新下载数据: 直接利用第4步中刚下载并保留在内存中的K线数据。
b. 计算差额: 检查这批新数据的数量是否满足内存容量要求 (kline_history_capacity，例如需要1000根，刚下载了50根)。
c. 按需从数据库补足: 如果数量不足，则计算还需要的K线数量（1000 - 50 = 950根），然后仅为这部分差额向SQLite发起一次查询，获取更早的950根历史K线。
d. 组合并存入内存: 将从数据库读取的旧数据和刚下载的新数据合并，形成一个完整有序的队列，然后存入 all_klines 结构中对应的位置。

启动服务:

所有数据均已加载到内存后，将数据结构的所有权移交给核心Actor任务。

启动命名管道服务器和（可选的）HTTP服务器。

服务正式就绪。

4. 架构决策与权衡

本次设计明确选择将数据补齐（ETL）过程作为服务启动的一部分，而非一个独立的外部程序。这是一个深思熟虑的决策。

选择的理由 (Pros):

部署简单: 整个服务是一个独立的单元，无需依赖外部调度任务（如 cronjob），降低了部署和运维的复杂性。

强一致性: 保证了每次服务成功启动时，其内存中的数据都是截至启动那一刻的最新状态。

接受的代价 (Cons):

启动时间较长: 服务的启动时间包含了完整的网络下载和数据库写入过程，可能会持续数分钟甚至更久。这对于需要快速重启的场景是一个挑战。

启动过程脆弱: 服务的成功启动强依赖于外部API（币安）的稳定性和网络连接质量。在这些外部条件不佳时，服务可能启动失败。

我们认为，对于一个以历史数据查询为核心的后台服务，数据的完整性和一致性的优先级高于服务的快速重启能力，因此当前的架构是合适的。

5. 公开API与配置
Generated toml
# config/memory_db_config.toml

# 预估的未来交易对总容量，用于一次性分配内存。
# 启动时会基于API获取的实际数量进行分配，此项可作为后备。
estimated_symbol_capacity = 4000

# 每个非日线周期在内存中保留的历史K线数量
kline_history_capacity = 10000

# 命名管道的全局唯一名称
pipe_name = "\\\\.\\pipe\\GlobalKlineFeed"

# SQLite数据库文件路径
database_path = "data/klines.db"
``````rust
// 消息定义 (保持不变)
pub enum DbCommand { /* ... */ }
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Toml
IGNORE_WHEN_COPYING_END
6. 性能与资源考量

启动时间: 这是本架构最重要的考量点。启动时间与品种数量、需要补齐的数据量、网络状况和API限制直接相关，是可变且可能较长的。运维层面必须为此做好准备。

内存占用: 内存使用是可预测和稳定的，主要由 实际品种数 * 周期数 * kline_history_capacity * size_of_kline 决定。

CPU使用: 启动阶段是I/O和CPU密集型。正常运行阶段CPU开销极低。

平台依赖: 依赖Windows命名管道。

容量监控: 服务需要监控 symbol_to_index 的大小，以便了解当前系统容纳的品种数量。