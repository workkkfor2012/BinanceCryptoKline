//! é«˜é¢‘æ—¥å¿—èšåˆå™¨æ¨¡å—
//!
//! åŸºäºevent_typeå­—æ®µçš„é«˜é¢‘æ—¥å¿—èšåˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
//! - åŸºäºæ—¶é—´çª—å£çš„é«˜é¢‘æ£€æµ‹ï¼ˆ5ç§’å†…2æ¬¡ï¼‰
//! - å­—æ®µå˜åŒ–å¯¹æ¯”
//! - O(1)æ€§èƒ½çš„HashMapèšåˆ

use crate::types::LogEntry;
use std::collections::{HashMap, VecDeque};
use serde::Serialize;
use chrono::{DateTime, Utc};

/// é«˜é¢‘æ—¥å¿—èšåˆé…ç½®
#[derive(Debug, Clone)]
pub struct HighFreqConfig {
    /// æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    pub time_window_seconds: i64,
    /// æœ€å°æ¬¡æ•°é˜ˆå€¼
    pub min_count_threshold: usize,
    /// æœ€å¤§ä¿å­˜æ¡ç›®æ•°
    pub max_entries: usize,
}

impl Default for HighFreqConfig {
    fn default() -> Self {
        Self {
            time_window_seconds: 5,     // 5ç§’çª—å£
            min_count_threshold: 2,     // 2æ¬¡å°±ç®—é«˜é¢‘
            max_entries: 1000,          // æœ€å¤šä¿å­˜1000ä¸ªä¸åŒçš„event_type
        }
    }
}

/// é«˜é¢‘æ—¥å¿—èšåˆæ¡ç›®
#[derive(Debug, Clone, Serialize)]
pub struct HighFreqLogEntry {
    pub event_type: String,
    pub count: usize,                    // æ€»æ¬¡æ•°
    pub latest_log: LogEntry,            // æœ€æ–°æ—¥å¿—
    pub previous_log: Option<LogEntry>,  // ä¸Šä¸€æ¡æ—¥å¿—ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    pub field_changes: HashMap<String, FieldChange>, // å­—æ®µå˜åŒ–
    pub last_updated: DateTime<Utc>,
    #[serde(skip)] // ä¸åºåˆ—åŒ–åˆ°å‰ç«¯
    pub timestamps: VecDeque<DateTime<Utc>>, // ç”¨äºæ—¶é—´çª—å£è®¡ç®—
}

/// å­—æ®µå˜åŒ–
#[derive(Debug, Clone, Serialize)]
pub struct FieldChange {
    pub field_name: String,
    pub previous_value: Option<serde_json::Value>,
    pub current_value: serde_json::Value,
    pub changed: bool,
}

/// é«˜é¢‘æ—¥å¿—èšåˆå™¨
#[derive(Debug)]
pub struct HighFreqLogAggregator {
    high_freq_logs: HashMap<String, HighFreqLogEntry>,
    config: HighFreqConfig,
}

impl HighFreqLogAggregator {
    /// åˆ›å»ºæ–°çš„é«˜é¢‘æ—¥å¿—èšåˆå™¨
    pub fn new(config: HighFreqConfig) -> Self {
        Self {
            high_freq_logs: HashMap::new(),
            config,
        }
    }

    /// å¤„ç†æ—¥å¿—æ¡ç›®ï¼Œè¿”å›æ˜¯å¦è¢«èšåˆ
    pub fn process_log_entry(&mut self, log_entry: LogEntry) -> bool {
        // æ£€æŸ¥æ˜¯å¦æ˜¯é«˜é¢‘æ—¥å¿—
        let event_type = log_entry.fields.get("event_type")
            .and_then(|v| v.as_str());
        let is_high_freq = log_entry.fields.get("is_high_freq")
            .and_then(|v| v.as_bool())
            .unwrap_or(false);

        // æ·»åŠ è°ƒè¯•æ—¥å¿—
        if log_entry.target == "buffered_kline_store" {
            println!("ğŸ” [é«˜é¢‘èšåˆå™¨] å¤„ç†æ—¥å¿—: target={}, event_type={:?}, is_high_freq={}, fields={:?}",
                log_entry.target, event_type, is_high_freq, log_entry.fields);
        }

        if let Some(event_type) = event_type {
            if is_high_freq {
                println!("âœ… [é«˜é¢‘èšåˆå™¨] æ£€æµ‹åˆ°é«˜é¢‘æ—¥å¿—: event_type={}, target={}", event_type, log_entry.target);
                return self.handle_high_freq_log(event_type.to_string(), log_entry);
            }
        }

        // ä¸æ˜¯é«˜é¢‘æ—¥å¿—ï¼Œä¸å¤„ç†
        false
    }

    /// å¤„ç†é«˜é¢‘æ—¥å¿—
    fn handle_high_freq_log(&mut self, event_type: String, log_entry: LogEntry) -> bool {
        let now = Utc::now();

        println!("ğŸ”„ [é«˜é¢‘èšåˆå™¨] å¤„ç†é«˜é¢‘æ—¥å¿—: event_type={}", event_type);

        // å…ˆæ£€æŸ¥æ˜¯å¦å­˜åœ¨ï¼Œé¿å…å€Ÿç”¨å†²çª
        let should_aggregate = if let Some(existing) = self.high_freq_logs.get_mut(&event_type) {
            // æ¸…ç†è¿‡æœŸçš„æ—¶é—´æˆ³ï¼ˆ5ç§’å¤–çš„ï¼‰
            let cutoff = now - chrono::Duration::seconds(self.config.time_window_seconds);
            let old_len = existing.timestamps.len();
            while let Some(&front_time) = existing.timestamps.front() {
                if front_time < cutoff {
                    existing.timestamps.pop_front();
                } else {
                    break;
                }
            }
            let new_len = existing.timestamps.len();
            if old_len != new_len {
                println!("ğŸ§¹ [é«˜é¢‘èšåˆå™¨] æ¸…ç†è¿‡æœŸæ—¶é—´æˆ³: {} -> {}", old_len, new_len);
            }

            // æ·»åŠ æ–°æ—¶é—´æˆ³
            existing.timestamps.push_back(now);
            let current_count = existing.timestamps.len();

            println!("ğŸ“Š [é«˜é¢‘èšåˆå™¨] æ—¶é—´çª—å£å†…è®¡æ•°: {}/{} (é˜ˆå€¼: {})",
                current_count, self.config.time_window_seconds, self.config.min_count_threshold);

            // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é«˜é¢‘é˜ˆå€¼ï¼ˆ5ç§’å†…2æ¬¡ï¼‰
            current_count >= self.config.min_count_threshold
        } else {
            println!("ğŸ†• [é«˜é¢‘èšåˆå™¨] é¦–æ¬¡é‡åˆ°æ­¤event_type: {}", event_type);
            false
        };

        if should_aggregate {
            // é‡æ–°è·å–å¯å˜å¼•ç”¨è¿›è¡Œæ›´æ–°
            if let Some(existing) = self.high_freq_logs.get_mut(&event_type) {
                // è®¡ç®—å­—æ®µå·®å¼‚
                let field_changes = Self::calculate_field_changes_static(&existing.latest_log, &log_entry);

                // æ›´æ–°èšåˆæ¡ç›®
                existing.previous_log = Some(existing.latest_log.clone());
                existing.latest_log = log_entry;
                existing.count += 1;
                existing.field_changes = field_changes;
                existing.last_updated = now;

                println!("ğŸ”„ [é«˜é¢‘èšåˆå™¨] èšåˆæ—¥å¿—: event_type={}, æ€»è®¡æ•°={}", event_type, existing.count);
                return true; // å·²èšåˆï¼Œä¸éœ€è¦å•ç‹¬æ˜¾ç¤º
            }
        } else if !self.high_freq_logs.contains_key(&event_type) {
            // åˆ›å»ºæ–°æ¡ç›®
            let mut timestamps = VecDeque::new();
            timestamps.push_back(now);

            let entry = HighFreqLogEntry {
                event_type: event_type.clone(),
                count: 1,
                latest_log: log_entry,
                previous_log: None,
                field_changes: HashMap::new(),
                last_updated: now,
                timestamps,
            };

            self.high_freq_logs.insert(event_type.clone(), entry);
            println!("ğŸ“ [é«˜é¢‘èšåˆå™¨] åˆ›å»ºæ–°èšåˆæ¡ç›®: event_type={}", event_type);
        }

        // è¿˜æ²¡è¾¾åˆ°é«˜é¢‘é˜ˆå€¼ï¼Œæ­£å¸¸æ˜¾ç¤º
        println!("â³ [é«˜é¢‘èšåˆå™¨] æœªè¾¾åˆ°èšåˆé˜ˆå€¼ï¼Œæ­£å¸¸æ˜¾ç¤ºæ—¥å¿—");
        false
    }

    /// è®¡ç®—å­—æ®µå˜åŒ–ï¼ˆé™æ€æ–¹æ³•é¿å…å€Ÿç”¨å†²çªï¼‰
    fn calculate_field_changes_static(
        previous: &LogEntry,
        current: &LogEntry
    ) -> HashMap<String, FieldChange> {
        let mut changes = HashMap::new();
        
        // æ¯”è¾ƒå½“å‰æ—¥å¿—çš„æ‰€æœ‰å­—æ®µ
        for (key, current_value) in &current.fields {
            let previous_value = previous.fields.get(key);
            let changed = match previous_value {
                Some(prev_val) => prev_val != current_value,
                None => true, // æ–°å­—æ®µç®—ä½œå˜åŒ–
            };
            
            changes.insert(key.clone(), FieldChange {
                field_name: key.clone(),
                previous_value: previous_value.cloned(),
                current_value: current_value.clone(),
                changed,
            });
        }
        
        changes
    }

    /// è·å–æ‰€æœ‰é«˜é¢‘æ—¥å¿—æ¡ç›®
    pub fn get_high_freq_logs(&self) -> Vec<HighFreqLogEntry> {
        self.high_freq_logs.values().cloned().collect()
    }

    /// æ¸…ç†è¿‡æœŸçš„é«˜é¢‘æ—¥å¿—æ¡ç›®
    pub fn cleanup_expired(&mut self, now: DateTime<Utc>) {
        let cutoff = now - chrono::Duration::minutes(30); // 30åˆ†é’Ÿåæ¸…ç†
        
        self.high_freq_logs.retain(|_, entry| {
            entry.last_updated > cutoff
        });
        
        // é™åˆ¶æ¡ç›®æ•°é‡
        if self.high_freq_logs.len() > self.config.max_entries {
            // ç®€å•ç­–ç•¥ï¼šæ¸…ç†æœ€è€çš„æ¡ç›®
            let mut entries: Vec<_> = self.high_freq_logs.iter()
                .map(|(k, v)| (k.clone(), v.last_updated))
                .collect();
            entries.sort_by_key(|(_, last_updated)| *last_updated);

            let to_remove = entries.len() - self.config.max_entries;
            for (event_type, _) in entries.iter().take(to_remove) {
                self.high_freq_logs.remove(event_type);
            }
        }
    }

    /// è·å–ç»Ÿè®¡ä¿¡æ¯
    pub fn get_stats(&self) -> HighFreqStats {
        HighFreqStats {
            total_event_types: self.high_freq_logs.len(),
            total_aggregated_logs: self.high_freq_logs.values()
                .map(|entry| entry.count)
                .sum(),
        }
    }
}

/// é«˜é¢‘æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize)]
pub struct HighFreqStats {
    pub total_event_types: usize,
    pub total_aggregated_logs: usize,
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashMap;

    fn create_test_log(event_type: &str, is_high_freq: bool, extra_fields: HashMap<String, serde_json::Value>) -> LogEntry {
        let mut fields = HashMap::new();
        fields.insert("event_type".to_string(), serde_json::Value::String(event_type.to_string()));
        fields.insert("is_high_freq".to_string(), serde_json::Value::Bool(is_high_freq));
        
        for (k, v) in extra_fields {
            fields.insert(k, v);
        }

        LogEntry {
            timestamp: Utc::now(),
            level: "INFO".to_string(),
            target: "test".to_string(),
            message: "test message".to_string(),
            module_path: None,
            file: None,
            line: None,
            fields,
            span: None,
        }
    }

    #[test]
    fn test_high_freq_aggregation() {
        let mut aggregator = HighFreqLogAggregator::new(HighFreqConfig::default());

        // ç¬¬ä¸€æ¬¡æ—¥å¿— - ä¸ä¼šè¢«èšåˆ
        let log1 = create_test_log("TEST_EVENT", true, {
            let mut fields = HashMap::new();
            fields.insert("count".to_string(), serde_json::Value::Number(serde_json::Number::from(1)));
            fields
        });

        assert!(!aggregator.process_log_entry(log1));
        assert_eq!(aggregator.high_freq_logs.len(), 1);

        // ç¬¬äºŒæ¬¡æ—¥å¿— - å¼€å§‹èšåˆ
        let log2 = create_test_log("TEST_EVENT", true, {
            let mut fields = HashMap::new();
            fields.insert("count".to_string(), serde_json::Value::Number(serde_json::Number::from(2)));
            fields
        });

        assert!(aggregator.process_log_entry(log2));

        let high_freq_logs = aggregator.get_high_freq_logs();
        assert_eq!(high_freq_logs.len(), 1);
        assert_eq!(high_freq_logs[0].count, 2);
        assert_eq!(high_freq_logs[0].event_type, "TEST_EVENT");
    }

    #[test]
    fn test_multiple_event_types_separate_aggregation() {
        let mut aggregator = HighFreqLogAggregator::new(HighFreqConfig::default());

        // æµ‹è¯•å¤šä¸ªä¸åŒçš„ event_type æ˜¯å¦åˆ†åˆ«èšåˆ

        // EVENT_A çš„ç¬¬ä¸€æ¬¡æ—¥å¿—
        let log_a1 = create_test_log("EVENT_A", true, {
            let mut fields = HashMap::new();
            fields.insert("value".to_string(), serde_json::Value::Number(serde_json::Number::from(100)));
            fields
        });
        assert!(!aggregator.process_log_entry(log_a1)); // é¦–æ¬¡ä¸èšåˆ

        // EVENT_B çš„ç¬¬ä¸€æ¬¡æ—¥å¿—
        let log_b1 = create_test_log("EVENT_B", true, {
            let mut fields = HashMap::new();
            fields.insert("value".to_string(), serde_json::Value::Number(serde_json::Number::from(200)));
            fields
        });
        assert!(!aggregator.process_log_entry(log_b1)); // é¦–æ¬¡ä¸èšåˆ

        // ç°åœ¨åº”è¯¥æœ‰ä¸¤ä¸ªä¸åŒçš„ event_type æ¡ç›®
        assert_eq!(aggregator.high_freq_logs.len(), 2);

        // EVENT_A çš„ç¬¬äºŒæ¬¡æ—¥å¿— - å¼€å§‹èšåˆ
        let log_a2 = create_test_log("EVENT_A", true, {
            let mut fields = HashMap::new();
            fields.insert("value".to_string(), serde_json::Value::Number(serde_json::Number::from(101)));
            fields
        });
        assert!(aggregator.process_log_entry(log_a2)); // å¼€å§‹èšåˆ

        // EVENT_B çš„ç¬¬äºŒæ¬¡æ—¥å¿— - å¼€å§‹èšåˆ
        let log_b2 = create_test_log("EVENT_B", true, {
            let mut fields = HashMap::new();
            fields.insert("value".to_string(), serde_json::Value::Number(serde_json::Number::from(201)));
            fields
        });
        assert!(aggregator.process_log_entry(log_b2)); // å¼€å§‹èšåˆ

        // éªŒè¯ä¸¤ä¸ª event_type åˆ†åˆ«èšåˆ
        let high_freq_logs = aggregator.get_high_freq_logs();
        assert_eq!(high_freq_logs.len(), 2);

        // æ‰¾åˆ°å¯¹åº”çš„èšåˆæ¡ç›®
        let event_a_entry = high_freq_logs.iter().find(|entry| entry.event_type == "EVENT_A").unwrap();
        let event_b_entry = high_freq_logs.iter().find(|entry| entry.event_type == "EVENT_B").unwrap();

        // éªŒè¯å„è‡ªçš„è®¡æ•°
        assert_eq!(event_a_entry.count, 2);
        assert_eq!(event_b_entry.count, 2);

        // éªŒè¯æœ€æ–°å€¼
        assert_eq!(event_a_entry.latest_log.fields.get("value").unwrap(), &serde_json::Value::Number(serde_json::Number::from(101)));
        assert_eq!(event_b_entry.latest_log.fields.get("value").unwrap(), &serde_json::Value::Number(serde_json::Number::from(201)));

        // ç»§ç»­æ·»åŠ  EVENT_A çš„ç¬¬ä¸‰æ¬¡æ—¥å¿—
        let log_a3 = create_test_log("EVENT_A", true, {
            let mut fields = HashMap::new();
            fields.insert("value".to_string(), serde_json::Value::Number(serde_json::Number::from(102)));
            fields
        });
        assert!(aggregator.process_log_entry(log_a3)); // ç»§ç»­èšåˆ

        // éªŒè¯åªæœ‰ EVENT_A çš„è®¡æ•°å¢åŠ äº†
        let high_freq_logs = aggregator.get_high_freq_logs();
        let event_a_entry = high_freq_logs.iter().find(|entry| entry.event_type == "EVENT_A").unwrap();
        let event_b_entry = high_freq_logs.iter().find(|entry| entry.event_type == "EVENT_B").unwrap();

        assert_eq!(event_a_entry.count, 3); // EVENT_A å¢åŠ åˆ° 3
        assert_eq!(event_b_entry.count, 2); // EVENT_B ä¿æŒ 2
    }
}
